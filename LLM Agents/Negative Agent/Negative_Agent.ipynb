{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Agent Fine-Tuning Using LLaMA-2 Model\n",
    "\n",
    "This notebook demonstrates how to fine-tune the LLaMA-2 model using the \"real-toxicity-prompts\" dataset. The goal is to enhance the model's negative prompts using advanced techniques like QLoRA and bitsandbytes quantization. \n",
    "\n",
    "We'll walk through each step, from installing necessary libraries, loading and preparing data, configuring the model, and finally, fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Necessary Libraries\n",
    "\n",
    "First, we need to install the necessary libraries such as `transformers`, `peft`, `bitsandbytes`, and others required for fine-tuning our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Necessary Packages\n",
    "\n",
    "Next, we import all the required packages including `datasets` for loading the dataset, `transformers` for model and tokenizer management, and other utilities like `torch` for tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Login to Hugging Face Hub\n",
    "\n",
    "Before proceeding, you need to log in to the Hugging Face Hub to access the models and datasets. Ensure you've logged in using your Hugging Face credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face Hub\n",
    "%huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Model and Dataset Parameters\n",
    "\n",
    "Here, we define the names of the model and dataset we'll use for fine-tuning. The base model is LLaMA-2, and we'll be using the \"real-toxicity-prompts\" dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and dataset names\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "dataset_name = \"allenai/real-toxicity-prompts\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set QLoRA Parameters\n",
    "\n",
    "In this step, we configure the parameters for QLoRA (Quantized Low-Rank Adaptation). These settings will control how the model handles attention layers and how it scales the LoRA layers during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension\n",
    "lora_r = 64  # Controls the rank of the LoRA layers, impacting model flexibility and efficiency\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16  # Scaling factor for the LoRA layers to control the adaptation strength\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1  # Introduce dropout to prevent overfitting in the LoRA layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure bitsandbytes Parameters\n",
    "\n",
    "We configure the bitsandbytes library parameters to handle 4-bit quantization. This allows us to load and train large models efficiently by reducing the memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True  # Enable 4-bit precision to save memory and speed up training\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"  # Data type for computations to balance precision and speed\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"  # Use 'nf4' quantization for potentially better performance\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False  # Disable nested quantization for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define Training Arguments\n",
    "\n",
    "We define the arguments that will control the training process, such as the number of epochs, learning rate, batch size, and other configurations necessary for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"  # Directory to save the trained model and checkpoints\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3  # Number of epochs to train the model, can be adjusted based on needs\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False  # Disable FP16 precision\n",
    "bf16 = False  # Disable BF16 precision\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4  # Number of samples processed per GPU during training\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4  # Number of samples processed per GPU during evaluation\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1  # Accumulate gradients over multiple steps to simulate larger batches\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True  # Enable to save memory during training\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3  # Gradient clipping to prevent exploding gradients\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4  # Starting learning rate for the optimizer\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001  # Regularization term to prevent overfitting\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"  # Optimizer type for training\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"  # Keep learning rate constant during training\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1  # Use the entire dataset (set to -1)\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03  # Fraction of steps used for learning rate warmup\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "group_by_length = True  # Efficient memory usage by grouping sequences of similar length\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25  # Frequency of saving model checkpoints\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25  # Frequency of logging training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Load and Prepare the Dataset\n",
    "\n",
    "We load the dataset using the `datasets` library and apply a custom function to generate negative prompts. This function will create more toxic prompts by adding negative language to the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Function to create negative prompts\n",
    "def create_negative_prompts(example):\n",
    "    # Find the highest toxicity score in the original prompt, handling None values\n",
    "    toxicity_values = [\n",
    "        example[\"prompt\"].get(\"toxicity\"),\n",
    "        example[\"prompt\"].get(\"severe_toxicity\"),\n",
    "        example[\"prompt\"].get(\"profanity\"),\n",
    "        example[\"prompt\"].get(\"sexually_explicit\"),\n",
    "        example[\"prompt\"].get(\"identity_attack\"),\n",
    "        example[\"prompt\"].get(\"flirtation\"),\n",
    "        example[\"prompt\"].get(\"threat\"),\n",
    "        example[\"prompt\"].get(\"insult\")\n",
    "    ]\n",
    "    # Remove None values before finding the maximum\n",
    "    toxicity_values = [v for v in toxicity_values if v is not None]\n",
    "    highest_toxicity = max(toxicity_values) if toxicity_values else 0  # Handle case with all None values\n",
    "\n",
    "    # Enhance the prompt with highly toxic language\n",
    "    new_prompt = {\n",
    "        \"text\": example[\"prompt\"][\"text\"] + \" and you should be ashamed of yourself.\",\n",
    "        \"profanity\": highest_toxicity,\n",
    "        \"sexually_explicit\": highest_toxicity,\n",
    "        \"identity_attack\": highest_toxicity,\n",
    "        \"flirtation\": highest_toxicity,\n",
    "        \"threat\": highest_toxicity,\n",
    "        \"insult\": highest_toxicity,\n",
    "        \"severe_toxicity\": highest_toxicity,\n",
    "        \"toxicity\": highest_toxicity\n",
    "    }\n",
    "\n",
    "    # Generate a continuation that reflects the most toxic outcome\n",
    "    new_continuation = {\n",
    "        \"severe_toxicity\": highest_toxicity,\n",
    "        \"toxicity\": highest_toxicity,\n",
    "        \"profanity\": highest_toxicity,\n",
    "        \"sexually_explicit\": highest_toxicity,\n",
    "        \"identity_attack\": highest_toxicity,\n",
    "        \"flirtation\": highest_toxicity,\n",
    "        \"threat\": highest_toxicity,\n",
    "        \"insult\": highest_toxicity\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"text\": new_prompt[\"text\"],  # Flattened text field\n",
    "        \"continuation\": new_continuation\n",
    "    }\n",
    "\n",
    "# Apply the negative prompt generation function to the dataset\n",
    "dataset = dataset.map(create_negative_prompts, batched=False)  # Use batched=False for simpler mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Load Tokenizer and Model with QLoRA Configuration\n",
    "\n",
    "We configure the tokenizer and the model with QLoRA settings. The model is loaded with 4-bit quantization to ensure efficient memory usage during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Step 3: Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Load LLaMA Tokenizer\n",
    "\n",
    "In this step, we load the tokenizer for the LLaMA model and configure it with special tokens for padding. This ensures that the input sequences are correctly tokenized and padded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Configure LoRA Settings\n",
    "\n",
    "We set up the LoRA (Low-Rank Adaptation) configuration, which controls how the LoRA layers are applied to the model during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Set Training Parameters and Initialize the Trainer\n",
    "\n",
    "We finalize the training parameters and initialize the `SFTTrainer`, which will handle the supervised fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Step 7: Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",  # Use the flattened text field\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Train and Save the Model\n",
    "\n",
    "Finally, we start the training process and save the fine-tuned model to the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Train model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
